{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d45a0482-a77e-46c6-b85a-26b55669c014",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import StructType, StructField, IntegerType, StringType, DateType, DoubleType\n",
    "from utils import Utils\n",
    "from utils.Utils import Transformer\n",
    "from utils.spark_utils import Spark_utils\n",
    "import logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "838aff5e-28af-4116-8759-a720b26e0e64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Environment:  DEV\n",
      "24/08/05 20:45:31 WARN Utils: Your hostname, EPCOBOGW1343 resolves to a loopback address: 127.0.1.1; using 10.255.255.254 instead (on interface lo)\n",
      "24/08/05 20:45:31 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24/08/05 20:45:32 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "## Settings variables for log handling and SparkSession\n",
    "spark = None\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "## Spark_utils object to enable SparkSession\n",
    "spark_util = Spark_utils(\"DEV\", \"Meli_Pipeline\")\n",
    "spark_util.set_spark_session()\n",
    "spark = spark_util.get_spark_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f8809778-666d-41d9-afb4-6d1356d87fb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Reading Data Sources from Local Storage...\n",
      "INFO:__main__:Defining prints.json schema ...\n",
      "INFO:__main__:Read prints.json with as dataset with proper schema\n",
      "INFO:__main__:Prints's dataframe schema...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- day: date (nullable = true)\n",
      " |-- event_data: struct (nullable = true)\n",
      " |    |-- position: integer (nullable = true)\n",
      " |    |-- value_prop: string (nullable = true)\n",
      " |-- user_id: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## Iniaiting data reads from original source to Dataframe\n",
    "logger.info(\"Reading Data Sources from Local Storage...\")\n",
    "\n",
    "logger.info(\"Defining prints.json schema ...\")\n",
    "prints_schema = StructType([\n",
    "StructField(\"day\", DateType(), True),\n",
    "StructField(\"event_data\", StructType([\n",
    "    StructField(\"position\", IntegerType(), True),\n",
    "    StructField(\"value_prop\", StringType(), True)\n",
    "]), True),\n",
    "StructField(\"user_id\", IntegerType(), True)\n",
    "])\n",
    "\n",
    "## Reading \"prints.json\" data to dataframe\n",
    "logger.info(\"Read prints.json with as dataset with proper schema\")\n",
    "df_prints = spark.read.json(\"data_sources/prints.json\", schema=prints_schema)\n",
    "\n",
    "logger.info(\"Prints's dataframe schema...\")\n",
    "df_prints.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "803a4577-cbc6-4ef8-a3aa-7c35266c4c1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:3 New Datasets were created\n"
     ]
    }
   ],
   "source": [
    "## Reading \"taps.json\" data to dataframe\n",
    "\n",
    "taps_schema = StructType([\n",
    "StructField(\"day\", DateType(), True),\n",
    "StructField(\"event_data\", StructType([\n",
    "    StructField(\"position\", IntegerType(), True),\n",
    "    StructField(\"value_prop\", StringType(), True)\n",
    "]), True),\n",
    "StructField(\"user_id\", IntegerType(), True)\n",
    "])\n",
    "    \n",
    "df_taps = spark.read.json(\"data_sources/taps.json\", schema = taps_schema)\n",
    "\n",
    "## Reading \"prints.json\" data to dataframe\n",
    "\n",
    "pays_schema = StructType([\n",
    "StructField(\"pay_date\", DateType(), True),\n",
    "StructField(\"total\", DoubleType(), True),\n",
    "StructField(\"user_id\", IntegerType(), True),\n",
    "StructField(\"value_prop\", StringType(), True)\n",
    "])\n",
    "    \n",
    "df_pays = spark.read.option(\"header\",\"True\").csv(\"data_sources/pays.csv\", schema = pays_schema)\n",
    "logger.info(\"3 New Datasets were created\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b44d2308-7900-4582-9510-5b7feb3a3fc6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Calculating week of year for column day in prints\n",
      "INFO:__main__:Calculating Rank for column week_of_year in prints\n",
      "INFO:__main__:Calculating Row_Number for column week_of_year in prints\n",
      "INFO:__main__:Showing processed data for Prints\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- day: date (nullable = true)\n",
      " |-- event_data: struct (nullable = true)\n",
      " |    |-- position: integer (nullable = true)\n",
      " |    |-- value_prop: string (nullable = true)\n",
      " |-- user_id: integer (nullable = true)\n",
      " |-- week_of_year: integer (nullable = true)\n",
      "\n",
      "root\n",
      " |-- day: date (nullable = true)\n",
      " |-- event_data: struct (nullable = true)\n",
      " |    |-- position: integer (nullable = true)\n",
      " |    |-- value_prop: string (nullable = true)\n",
      " |-- user_id: integer (nullable = true)\n",
      " |-- week_of_year: integer (nullable = true)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Calculating week of year for column day in taps\n",
      "INFO:__main__:Calculating Rank for column week_of_year in taps\n",
      "INFO:__main__:Calculating Row_Number for column week_of_year in prints\n",
      "INFO:__main__:Showing processed data for Prints\n",
      "INFO:__main__:Calculating week of year for column day in pays\n",
      "INFO:__main__:Calculating Rank for column week_of_year in pays\n",
      "INFO:__main__:Calculating Row_Number for column week_of_year in prints\n",
      "INFO:__main__:Showing processed data for Prints\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----------------------+-------+------------+\n",
      "|day       |event_data             |user_id|week_of_year|\n",
      "+----------+-----------------------+-------+------------+\n",
      "|2020-11-01|{0, cellphone_recharge}|98702  |44          |\n",
      "|2020-11-01|{1, prepaid}           |98702  |44          |\n",
      "|2020-11-01|{0, prepaid}           |63252  |44          |\n",
      "|2020-11-01|{0, cellphone_recharge}|24728  |44          |\n",
      "|2020-11-01|{1, link_cobro}        |24728  |44          |\n",
      "|2020-11-01|{2, credits_consumer}  |24728  |44          |\n",
      "|2020-11-01|{3, point}             |24728  |44          |\n",
      "|2020-11-01|{0, point}             |25517  |44          |\n",
      "|2020-11-01|{1, credits_consumer}  |25517  |44          |\n",
      "|2020-11-01|{2, transport}         |25517  |44          |\n",
      "|2020-11-01|{0, point}             |57587  |44          |\n",
      "|2020-11-01|{0, transport}         |13609  |44          |\n",
      "|2020-11-01|{0, cellphone_recharge}|3708   |44          |\n",
      "|2020-11-01|{1, prepaid}           |3708   |44          |\n",
      "|2020-11-01|{2, point}             |3708   |44          |\n",
      "|2020-11-01|{3, send_money}        |3708   |44          |\n",
      "|2020-11-01|{0, send_money}        |99571  |44          |\n",
      "|2020-11-01|{1, point}             |99571  |44          |\n",
      "|2020-11-01|{2, link_cobro}        |99571  |44          |\n",
      "|2020-11-01|{0, prepaid}           |53823  |44          |\n",
      "+----------+-----------------------+-------+------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "root\n",
      " |-- day: date (nullable = true)\n",
      " |-- event_data: struct (nullable = true)\n",
      " |    |-- position: integer (nullable = true)\n",
      " |    |-- value_prop: string (nullable = true)\n",
      " |-- user_id: integer (nullable = true)\n",
      " |-- week_of_year: integer (nullable = true)\n",
      "\n",
      "root\n",
      " |-- day: date (nullable = true)\n",
      " |-- event_data: struct (nullable = true)\n",
      " |    |-- position: integer (nullable = true)\n",
      " |    |-- value_prop: string (nullable = true)\n",
      " |-- user_id: integer (nullable = true)\n",
      " |-- week_of_year: integer (nullable = true)\n",
      "\n",
      "+----------+-----------------------+-------+------------+\n",
      "|day       |event_data             |user_id|week_of_year|\n",
      "+----------+-----------------------+-------+------------+\n",
      "|2020-11-01|{0, cellphone_recharge}|98702  |44          |\n",
      "|2020-11-01|{2, point}             |3708   |44          |\n",
      "|2020-11-01|{3, send_money}        |3708   |44          |\n",
      "|2020-11-01|{0, transport}         |93963  |44          |\n",
      "|2020-11-01|{1, cellphone_recharge}|93963  |44          |\n",
      "|2020-11-01|{0, link_cobro}        |94945  |44          |\n",
      "|2020-11-01|{1, cellphone_recharge}|94945  |44          |\n",
      "|2020-11-01|{2, prepaid}           |89026  |44          |\n",
      "|2020-11-01|{0, link_cobro}        |7616   |44          |\n",
      "|2020-11-01|{0, link_cobro}        |63471  |44          |\n",
      "|2020-11-01|{1, send_money}        |98277  |44          |\n",
      "|2020-11-01|{2, cellphone_recharge}|83634  |44          |\n",
      "|2020-11-01|{0, cellphone_recharge}|2367   |44          |\n",
      "|2020-11-01|{0, credits_consumer}  |14472  |44          |\n",
      "|2020-11-01|{2, send_money}        |88316  |44          |\n",
      "|2020-11-01|{0, credits_consumer}  |40910  |44          |\n",
      "|2020-11-01|{0, cellphone_recharge}|412    |44          |\n",
      "|2020-11-01|{1, prepaid}           |76198  |44          |\n",
      "|2020-11-01|{3, transport}         |52715  |44          |\n",
      "|2020-11-01|{2, credits_consumer}  |9294   |44          |\n",
      "+----------+-----------------------+-------+------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "root\n",
      " |-- pay_date: date (nullable = true)\n",
      " |-- total: double (nullable = true)\n",
      " |-- user_id: integer (nullable = true)\n",
      " |-- value_prop: string (nullable = true)\n",
      " |-- week_of_year: integer (nullable = true)\n",
      "\n",
      "root\n",
      " |-- pay_date: date (nullable = true)\n",
      " |-- total: double (nullable = true)\n",
      " |-- user_id: integer (nullable = true)\n",
      " |-- value_prop: string (nullable = true)\n",
      " |-- week_of_year: integer (nullable = true)\n",
      "\n",
      "+----------+------+-------+------------------+------------+\n",
      "|pay_date  |total |user_id|value_prop        |week_of_year|\n",
      "+----------+------+-------+------------------+------------+\n",
      "|2020-11-01|7.04  |35994  |link_cobro        |44          |\n",
      "|2020-11-01|37.36 |79066  |cellphone_recharge|44          |\n",
      "|2020-11-01|15.84 |19321  |cellphone_recharge|44          |\n",
      "|2020-11-01|26.26 |19321  |send_money        |44          |\n",
      "|2020-11-01|35.35 |38438  |send_money        |44          |\n",
      "|2020-11-01|20.95 |85939  |transport         |44          |\n",
      "|2020-11-01|74.48 |14372  |prepaid           |44          |\n",
      "|2020-11-01|31.52 |14372  |link_cobro        |44          |\n",
      "|2020-11-01|83.76 |65274  |transport         |44          |\n",
      "|2020-11-01|93.54 |65274  |prepaid           |44          |\n",
      "|2020-11-01|37.84 |97428  |link_cobro        |44          |\n",
      "|2020-11-01|26.77 |82163  |link_cobro        |44          |\n",
      "|2020-11-01|92.56 |9816   |send_money        |44          |\n",
      "|2020-11-01|122.03|9816   |prepaid           |44          |\n",
      "|2020-11-01|83.66 |28929  |prepaid           |44          |\n",
      "|2020-11-01|136.78|97275  |link_cobro        |44          |\n",
      "|2020-11-01|17.34 |85001  |cellphone_recharge|44          |\n",
      "|2020-11-01|41.93 |85001  |link_cobro        |44          |\n",
      "|2020-11-01|31.38 |42     |send_money        |44          |\n",
      "|2020-11-01|39.28 |42     |transport         |44          |\n",
      "+----------+------+-------+------------------+------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#################### Processing df_prints DataFrame ##########################\n",
    "\n",
    "logger.info(\"Calculating week of year for column day in prints\")\n",
    "df_prints = Transformer.calcWeekOfTheYear(df_prints, \"day\", \"week_of_year\")\n",
    "\n",
    "df_prints.printSchema()\n",
    "\n",
    "logger.info(\"Calculating Rank for column week_of_year in prints\")\n",
    "##df_prints = Transformer.calcRank(df_prints, ['user_id'], ['week_of_year'], \"rank_num\", \"desc\")\n",
    "\n",
    "logger.info(\"Calculating Row_Number for column week_of_year in prints\")\n",
    "##df_prints = Transformer.calcRowNumber(df_prints, ['user_id','week_of_year'], ['week_of_year'], \"row_num\", \"desc\")\n",
    "    \n",
    "df_prints.printSchema()\n",
    "\n",
    "logger.info(\"Showing processed data for Prints\")\n",
    "df_prints.show(truncate = False)\n",
    "##df_prints.show(10, truncate = False)\n",
    "\n",
    "#################### Processing df_taps DataFrame ##########################\n",
    "\n",
    "logger.info(\"Calculating week of year for column day in taps\")\n",
    "df_taps = Transformer.calcWeekOfTheYear(df_taps, \"day\", \"week_of_year\")\n",
    "\n",
    "df_taps.printSchema()\n",
    "\n",
    "logger.info(\"Calculating Rank for column week_of_year in taps\")\n",
    "##df_taps = Transformer.calcRank(df_taps, ['user_id'], ['week_of_year'], \"rank_num\", \"desc\")\n",
    "\n",
    "logger.info(\"Calculating Row_Number for column week_of_year in prints\")\n",
    "##df_taps = Transformer.calcRowNumber(df_taps, ['user_id','week_of_year'], ['week_of_year'], \"row_num\", \"desc\")\n",
    "    \n",
    "df_taps.printSchema()\n",
    "\n",
    "logger.info(\"Showing processed data for Prints\")\n",
    "df_taps.show(truncate = False)\n",
    "##df_prints.show(10, truncate = False)\n",
    "\n",
    "#################### Processing df_pays DataFrame ##########################\n",
    "\n",
    "logger.info(\"Calculating week of year for column day in pays\")\n",
    "df_pays = Transformer.calcWeekOfTheYear(df_pays, \"pay_date\", \"week_of_year\")\n",
    "\n",
    "df_pays.printSchema()\n",
    "\n",
    "logger.info(\"Calculating Rank for column week_of_year in pays\")\n",
    "##df_pays = Transformer.calcRank(df_pays, ['user_id'], ['week_of_year'], \"rank_num\", \"desc\")\n",
    "\n",
    "logger.info(\"Calculating Row_Number for column week_of_year in prints\")\n",
    "##df_pays = Transformer.calcRowNumber(df_pays, ['user_id','week_of_year'], ['week_of_year'], \"row_num\", \"desc\")\n",
    "    \n",
    "df_pays.printSchema()\n",
    "\n",
    "logger.info(\"Showing processed data for Prints\")\n",
    "df_pays.show(truncate = False)\n",
    "##df_prints.show(10, truncate = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7159c0d6-61b9-4840-95a0-ec4b4a5c8b1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_prints.createOrReplaceTempView(\"prints\")\n",
    "df_taps.createOrReplaceTempView(\"taps\")\n",
    "df_pays.createOrReplaceTempView(\"pays\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef53c84b-7de3-4d75-afca-704a0bb79e0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"\"\"\n",
    "select user_id,\n",
    "count(1) q ,\n",
    "min(week_of_year),\n",
    "max(week_of_year)\n",
    "from prints\n",
    "group by user_id\n",
    "order by q desc\n",
    "\"\"\").show(truncate = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e963e99-3b9c-4377-982e-42c3e26c7040",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"\"\"\n",
    "select count(1) q ,\n",
    "min(week_of_year),\n",
    "avg(week_of_year),\n",
    "max(week_of_year)\n",
    "from prints\n",
    "order by q desc\n",
    "\"\"\").show(truncate = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cd10cd6-4f6d-4e6f-833a-a133da9d0b76",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"\"\"\n",
    "select x.*\n",
    "from\n",
    "(select * ,\n",
    "rank() over (partition by user_id order by week_of_year desc) rown\n",
    "from prints\n",
    "where user_id = 1) x\n",
    "\"\"\").show(truncate = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c762e45-13ea-43c1-8c2a-7900845e845e",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"\"\"\n",
    "SELECT a, b, row_number() OVER (PARTITION BY a ORDER BY b) FROM VALUES ('A1', 2), ('A1', 1), ('A2', 3), ('A1', 1) tab(a, b);\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a2893a2d-5001-4ac1-9b2e-e1e25adb8a76",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 3:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------+---+\n",
      "|user_id|week_of_year|q  |\n",
      "+-------+------------+---+\n",
      "|1      |48          |2  |\n",
      "|1      |45          |2  |\n",
      "|2      |48          |3  |\n",
      "|2      |46          |1  |\n",
      "|2      |45          |5  |\n",
      "|3      |47          |5  |\n",
      "|3      |46          |3  |\n",
      "|3      |45          |1  |\n",
      "|4      |49          |1  |\n",
      "|4      |48          |1  |\n",
      "|4      |47          |1  |\n",
      "|4      |46          |2  |\n",
      "|4      |45          |1  |\n",
      "|5      |48          |2  |\n",
      "|5      |47          |1  |\n",
      "|5      |45          |5  |\n",
      "|6      |46          |3  |\n",
      "|6      |45          |2  |\n",
      "|7      |48          |2  |\n",
      "|7      |46          |2  |\n",
      "+-------+------------+---+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "select user_id\n",
    ",week_of_year\n",
    ",count(1) q\n",
    "from prints\n",
    "group by user_id\n",
    ",week_of_year\n",
    "order by user_id , week_of_year desc\n",
    "\"\"\").show(truncate = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cf7f92e3-8baa-41f4-962d-5ec733a65726",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---+\n",
      "|user_id|q  |\n",
      "+-------+---+\n",
      "|35156  |27 |\n",
      "|9704   |27 |\n",
      "|61554  |26 |\n",
      "|88770  |26 |\n",
      "|30781  |25 |\n",
      "|51870  |25 |\n",
      "|5352   |25 |\n",
      "|48782  |25 |\n",
      "|64536  |24 |\n",
      "|38850  |24 |\n",
      "|33842  |24 |\n",
      "|56578  |24 |\n",
      "|59876  |24 |\n",
      "|65036  |24 |\n",
      "|41940  |24 |\n",
      "|88191  |24 |\n",
      "|20457  |24 |\n",
      "|50813  |24 |\n",
      "|95769  |24 |\n",
      "|19656  |24 |\n",
      "+-------+---+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "select user_id\n",
    ",count(1) q\n",
    "from prints\n",
    "group by user_id\n",
    "order by q desc  \n",
    "\"\"\").show(truncate = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "36da2040-6312-4ce7-8baf-8572d031c946",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 24:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----------------------+-------+------------+-----+\n",
      "|day       |event_data             |user_id|week_of_year|click|\n",
      "+----------+-----------------------+-------+------------+-----+\n",
      "|2020-11-01|{0, cellphone_recharge}|3708   |44          |n    |\n",
      "|2020-11-01|{1, prepaid}           |3708   |44          |n    |\n",
      "|2020-11-01|{2, point}             |3708   |44          |y    |\n",
      "|2020-11-01|{3, send_money}        |3708   |44          |y    |\n",
      "|2020-11-09|{0, credits_consumer}  |3708   |46          |n    |\n",
      "|2020-11-13|{0, link_cobro}        |3708   |46          |n    |\n",
      "|2020-11-25|{0, link_cobro}        |3708   |48          |n    |\n",
      "|2020-11-25|{1, transport}         |3708   |48          |n    |\n",
      "+----------+-----------------------+-------+------------+-----+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "select a.*\n",
    ",case when b.user_id is not null then \"y\" else \"n\" end click\n",
    "from prints a\n",
    "left join taps b\n",
    "on a.user_id = b.user_id\n",
    "and a.event_data = b.event_data\n",
    "and a.day = b.day\n",
    "where a.user_id = 3708\n",
    "\"\"\").show(30, truncate = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5457994b-0f47-4b11-90c8-6662578e3e3a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
